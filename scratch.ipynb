{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "690b705c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# siamese_lfp_model.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf93163b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Config ------------\n",
    "SEGMENT_LENGTH = 24000  # 1 second @ 24kHz\n",
    "BATCH_SIZE = 256\n",
    "EMBEDDING_DIM = 128\n",
    "LEARNING_RATE = 1e-4\n",
    "EPOCHS = 200\n",
    "LOG_DIR = \"runs/siamese_lfp3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad389620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Data Preparation ------------\n",
    "def segment_data(data, segment_length):\n",
    "    num_segments = len(data) // segment_length\n",
    "    segments = np.array(np.split(data[:num_segments * segment_length], num_segments))\n",
    "    np.random.shuffle(segments)\n",
    "    return segments\n",
    "\n",
    "\n",
    "# def create_pairs(gpi_segments, stn_segments):\n",
    "#     pairs = []\n",
    "#     labels = []\n",
    "\n",
    "#     # GPi-GPi (label 0)\n",
    "#     for i in range(len(gpi_segments) - 1):\n",
    "#         pairs.append((gpi_segments[i], gpi_segments[i+1]))\n",
    "#         labels.append(0)\n",
    "\n",
    "#     # STN-STN (label 0)\n",
    "#     for i in range(len(stn_segments) - 1):\n",
    "#         pairs.append((stn_segments[i], stn_segments[i+1]))\n",
    "#         labels.append(0)\n",
    "\n",
    "#     # GPi-STN (label 1)\n",
    "#     for i in range(min(len(gpi_segments), len(stn_segments))):\n",
    "#         pairs.append((gpi_segments[i], stn_segments[i]))\n",
    "#         labels.append(1)\n",
    "\n",
    "#     ratio_similar = labels.count(0) / len(labels)\n",
    "#     print(f\"Similarity label ratio: {ratio_similar:.2f} similar (label 0), {1 - ratio_similar:.2f} dissimilar (label 1)\")\n",
    "\n",
    "#     return pairs, labels\n",
    "\n",
    "def create_pairs(gpi_segments, stn_segments):\n",
    "    # GPi-GPi (label 0)\n",
    "    gpi_gpi_pairs = [(gpi_segments[i], gpi_segments[i+1]) for i in range(len(gpi_segments) - 1)]\n",
    "    gpi_gpi_labels = [0] * len(gpi_gpi_pairs)\n",
    "\n",
    "    # STN-STN (label 0)\n",
    "    stn_stn_pairs = [(stn_segments[i], stn_segments[i+1]) for i in range(len(stn_segments) - 1)]\n",
    "    stn_stn_labels = [0] * len(stn_stn_pairs)\n",
    "\n",
    "    # GPi-STN (label 1)\n",
    "    gpi_stn_pairs = [(gpi_segments[i], stn_segments[i]) for i in range(min(len(gpi_segments), len(stn_segments)))]\n",
    "    gpi_stn_labels = [1] * len(gpi_stn_pairs)\n",
    "\n",
    "    # Combine similar pairs\n",
    "    similar_pairs = gpi_gpi_pairs + stn_stn_pairs\n",
    "    similar_labels = gpi_gpi_labels + stn_stn_labels\n",
    "\n",
    "    # Balance both classes\n",
    "    min_len = min(len(similar_pairs), len(gpi_stn_pairs))\n",
    "    balanced_pairs = similar_pairs[:min_len] + gpi_stn_pairs[:min_len]\n",
    "    balanced_labels = similar_labels[:min_len] + gpi_stn_labels[:min_len]\n",
    "\n",
    "    ratio_similar = balanced_labels.count(0) / len(balanced_labels)\n",
    "    print(f\"Balanced similarity label ratio: {ratio_similar:.2f} similar (label 0), {1 - ratio_similar:.2f} dissimilar (label 1)\")\n",
    "\n",
    "    return balanced_pairs, balanced_labels\n",
    "\n",
    "\n",
    "class LFPDataset(Dataset):\n",
    "    def __init__(self, pairs, labels):\n",
    "        self.pairs = pairs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x1, x2 = self.pairs[idx]\n",
    "        x1 = torch.tensor(x1, dtype=torch.float32).unsqueeze(0)\n",
    "        x2 = torch.tensor(x2, dtype=torch.float32).unsqueeze(0)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        return x1, x2, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b0c84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Model Definition ------------\n",
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNNEncoder, self).__init__()\n",
    "        # self.layers = nn.ModuleList([\n",
    "        #     nn.Dropout(0.3),\n",
    "        #     nn.Conv1d(1, 4, kernel_size=16, stride=2, padding=8),\n",
    "        #     nn.BatchNorm1d(4),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Dropout(0.3),\n",
    "        #     nn.Conv1d(4, 8, kernel_size=32, stride=2, padding=16),\n",
    "        #     nn.BatchNorm1d(8),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Dropout(0.3),\n",
    "        #     nn.Conv1d(8, 16, kernel_size=64, stride=2, padding=32),\n",
    "        #     nn.BatchNorm1d(16),\n",
    "        #     nn.ReLU(),\n",
    "        #     # nn.Dropout(0.1),\n",
    "        #     nn.Conv1d(16, 32, kernel_size=128, stride=2, padding=64),\n",
    "        #     nn.BatchNorm1d(32),\n",
    "        #     nn.ReLU(),\n",
    "        #     # nn.Dropout(0.1),\n",
    "        #     nn.Conv1d(32, 64, kernel_size=256, stride=2, padding=128),\n",
    "        #     nn.BatchNorm1d(64),\n",
    "        #     nn.ReLU(),\n",
    "        #     # nn.Dropout(0.1),\n",
    "        #     nn.Conv1d(64, 64, kernel_size=512, stride=2, padding=256),\n",
    "        #     nn.BatchNorm1d(64),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Dropout(0.3),\n",
    "        #     nn.AdaptiveAvgPool1d(1)\n",
    "        # ])\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Conv1d(1, 32, kernel_size=32, stride=8, padding=0),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Conv1d(32, 64, kernel_size=32, stride=8, padding=0),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Conv1d(64, 64, kernel_size=16, stride=4, padding=0),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Conv1d(64, 64, kernel_size=8, stride=2, padding=0),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Conv1d(64, 64, kernel_size=4, stride=2, padding=0),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Conv1d(64, 64, kernel_size=4, stride=2, padding=0),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "        ])\n",
    "        self.fc = nn.Linear(64, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    total = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total trainable parameters: {total:,}\\n\")\n",
    "    print(\"Trainable parameters by layer:\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f\"{name:50} {param.numel():,}\")\n",
    "\n",
    "\n",
    "def print_model_summary(model, train_loader, device):\n",
    "    print(\"===== Sample Forward Pass Shape Info =====\")\n",
    "    x_sample = next(iter(train_loader))[0].to(device)\n",
    "    model.encoder(x_sample)\n",
    "    print(\"\\nModel Summary:\\n\")\n",
    "    print(model)\n",
    "    count_parameters(model)\n",
    "\n",
    "\n",
    "class SiameseNet(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(SiameseNet, self).__init__()\n",
    "        self.encoder = CNNEncoder(embedding_dim)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        embed1 = self.encoder(x1)\n",
    "        embed2 = self.encoder(x2)\n",
    "        distance = F.pairwise_distance(embed1, embed2)\n",
    "        return distance \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0ee84f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------- Training Loop ------------\n",
    "def train(model, dataloader, optimizer, criterion, device, writer, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (x1, x2, label) in enumerate(dataloader):\n",
    "        x1, x2, label = x1.to(device), x2.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x1, x2)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        preds = (output > 0.5).float()\n",
    "        correct += (preds == label).sum().item()\n",
    "        total += label.size(0)\n",
    "        if i == 0:\n",
    "            print(\"Preds:\", preds[:10])\n",
    "            print(\"Labels:\", label[:10])\n",
    "\n",
    "        writer.add_scalar('Train/Batch_Loss', loss.item(), epoch * len(dataloader) + i)\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    accuracy = correct / total\n",
    "    print(f\"Epoch {epoch+1} Train Accuracy: {accuracy:.4f}\")\n",
    "    writer.add_scalar('Train/Epoch_Loss', avg_loss, epoch)\n",
    "    writer.add_scalar('Train/Accuracy', accuracy, epoch)\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device, writer, epoch):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x1, x2, label in dataloader:\n",
    "            x1, x2, label = x1.to(device), x2.to(device), label.to(device)\n",
    "            output = model(x1, x2)\n",
    "            loss = criterion(output, label)\n",
    "            total_loss += loss.item()\n",
    "            preds = (output > 0.5).float()  # Same for both train and test\n",
    "            correct += (preds == label).sum().item()\n",
    "            total += label.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct / total\n",
    "    print(f\"Epoch {epoch+1} Test Accuracy: {accuracy:.4f}\")\n",
    "    writer.add_scalar('Test/Epoch_Loss', avg_loss, epoch)\n",
    "    writer.add_scalar('Test/Accuracy', accuracy, epoch)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57497af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Run Pipeline ------------\n",
    "def main():\n",
    "    # Print dataset shape and sizes\n",
    "    print(\"===== Dataset Information =====\")\n",
    "    # TensorBoard setup\n",
    "    os.makedirs(LOG_DIR, exist_ok=True)\n",
    "    writer = SummaryWriter(LOG_DIR)\n",
    "    # # Simulate random LFP-like signals\n",
    "    # GPiData = np.random.randn(24000 * 60 * 2)  # 2 minutes of data\n",
    "    # STNData = np.random.randn(24000 * 60 * 2)\n",
    "    # Simulate random LFP-like signals\n",
    "    # Generate synthetic LFP-like signals with sinusoids + noise\n",
    "\n",
    "    # fs = 24000\n",
    "    # duration_sec = 60 * 2\n",
    "    # t = np.linspace(0, duration_sec, fs * duration_sec, endpoint=False)\n",
    "\n",
    "    # gpi_signal = np.sin(2 * np.pi * 10 * t)  # 10 Hz sinusoid\n",
    "    # stn_signal = np.sin(2 * np.pi * 10 * t)  # 30 Hz sinusoid\n",
    "#ashaks\n",
    "    # GPiData = gpi_signal + 0.5 * np.random.randn(len(t))\n",
    "    # STNData = stn_signal + 0.5 * np.random.randn(len(t))\n",
    "\n",
    "    with h5py.File(\"F:\\Python Projects\\data\\period9\\microGPi1_L_1_CommonFiltered.mat\", \"r\") as f:\n",
    "        GPiData = np.array(f[\"data\"]).squeeze()\n",
    "        fs = int(np.array(f[\"fs\"]).squeeze())\n",
    "        # rtesting\n",
    "\n",
    "    # for Sina's PC: F:\\Python Projects\\data\\period9\n",
    "    # for shared PC: D:\\Sina\\Data\\period9\\microSTN_L_3_CommonFiltered.mat\n",
    "    with h5py.File(\"F:\\Python Projects\\data\\period9\\microSTN_L_3_CommonFiltered.mat\", \"r\") as f:\n",
    "        STNData = np.array(f[\"data\"]).squeeze()  # replace with your actual filename\n",
    "\n",
    "    # Segment\n",
    "    gpi_segments = segment_data(GPiData, SEGMENT_LENGTH)\n",
    "    stn_segments = segment_data(STNData, SEGMENT_LENGTH)\n",
    "    pairs, labels = create_pairs(gpi_segments, stn_segments)\n",
    "\n",
    "    # Split data\n",
    "    # train_pairs, test_pairs, train_labels, test_labels = train_test_split(pairs, labels, test_size=0.2, stratify=labels)\n",
    "    train_pairs, test_pairs, train_labels, test_labels = train_test_split( pairs, labels, test_size=0.2, stratify=labels, shuffle=True, random_state=42)\n",
    "\n",
    "    # Dataloaders\n",
    "    train_ds = LFPDataset(train_pairs, train_labels)\n",
    "    test_ds = LFPDataset(test_pairs, test_labels)\n",
    "    print(f\"Train set: {len(train_ds)} pairs\")\n",
    "    print(f\"Test set: {len(test_ds)} pairs\")\n",
    "    print(f\"Batch size: {BATCH_SIZE}\")\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE)\n",
    "\n",
    "    # Model setup\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "    model = SiameseNet(embedding_dim=EMBEDDING_DIM).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    print_model_summary(model, train_loader, device)\n",
    "    print(\"===== Training Input Shape Info =====\")\n",
    "    for x1, x2, _ in train_loader:\n",
    "        print(f\"Train input shapes: x1: {x1.shape}, x2: {x2.shape}\")\n",
    "        break\n",
    "    print(\"===== Testing Input Shape Info =====\")\n",
    "\n",
    "    def contrastive_loss(distances, labels, margin=1.0):\n",
    "        labels = labels.float()\n",
    "        loss_similar = (1 - labels) * distances.pow(2)\n",
    "        loss_dissimilar = labels * F.relu(margin - distances).pow(2)\n",
    "        return torch.mean(loss_similar + loss_dissimilar)\n",
    "\n",
    "    criterion = contrastive_loss\n",
    "    for x1, x2, _ in test_loader:\n",
    "        print(f\"Test input shapes: x1: {x1.shape}, x2: {x2.shape}\")\n",
    "        break\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "\n",
    "    \n",
    "        train_loss = train(model, train_loader, optimizer, criterion, device, writer, epoch)\n",
    "        test_loss = evaluate(model, test_loader, criterion, device, writer, epoch)\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    writer.close()\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f4d671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Dataset Information =====\n",
      "Balanced similarity label ratio: 0.50 similar (label 0), 0.50 dissimilar (label 1)\n",
      "Train set: 6177 pairs\n",
      "Test set: 1545 pairs\n",
      "Batch size: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.4 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"C:\\Program Files\\Python310\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Program Files\\Python310\\lib\\runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"d:\\Sina\\NIPS2025\\NIPS2025\\venv\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"d:\\Sina\\NIPS2025\\NIPS2025\\venv\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"d:\\Sina\\NIPS2025\\NIPS2025\\venv\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"d:\\Sina\\NIPS2025\\NIPS2025\\venv\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Program Files\\Python310\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Program Files\\Python310\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Program Files\\Python310\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"d:\\Sina\\NIPS2025\\NIPS2025\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"d:\\Sina\\NIPS2025\\NIPS2025\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"d:\\Sina\\NIPS2025\\NIPS2025\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"d:\\Sina\\NIPS2025\\NIPS2025\\venv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"d:\\Sina\\NIPS2025\\NIPS2025\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"d:\\Sina\\NIPS2025\\NIPS2025\\venv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"d:\\Sina\\NIPS2025\\NIPS2025\\venv\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"d:\\Sina\\NIPS2025\\NIPS2025\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3077, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"d:\\Sina\\NIPS2025\\NIPS2025\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3132, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"d:\\Sina\\NIPS2025\\NIPS2025\\venv\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"d:\\Sina\\NIPS2025\\NIPS2025\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3336, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"d:\\Sina\\NIPS2025\\NIPS2025\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3519, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"d:\\Sina\\NIPS2025\\NIPS2025\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3579, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Maral\\AppData\\Local\\Temp\\ipykernel_9456\\1665282582.py\", line 3, in <module>\n",
      "    model = main()\n",
      "  File \"C:\\Users\\Maral\\AppData\\Local\\Temp\\ipykernel_9456\\3313568080.py\", line 53, in main\n",
      "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "C:\\Users\\Maral\\AppData\\Local\\Temp\\ipykernel_9456\\3313568080.py:53: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:68.)\n",
      "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Sample Forward Pass Shape Info =====\n",
      "\n",
      "Model Summary:\n",
      "\n",
      "SiameseNet(\n",
      "  (encoder): CNNEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): Dropout(p=0.3, inplace=False)\n",
      "      (1): Conv1d(1, 4, kernel_size=(16,), stride=(2,), padding=(8,))\n",
      "      (2): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ReLU()\n",
      "      (4): Dropout(p=0.3, inplace=False)\n",
      "      (5): Conv1d(4, 8, kernel_size=(32,), stride=(2,), padding=(16,))\n",
      "      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (7): ReLU()\n",
      "      (8): Dropout(p=0.3, inplace=False)\n",
      "      (9): Conv1d(8, 16, kernel_size=(64,), stride=(2,), padding=(32,))\n",
      "      (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (11): ReLU()\n",
      "      (12): Conv1d(16, 32, kernel_size=(128,), stride=(2,), padding=(64,))\n",
      "      (13): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (14): ReLU()\n",
      "      (15): Conv1d(32, 64, kernel_size=(256,), stride=(2,), padding=(128,))\n",
      "      (16): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (17): ReLU()\n",
      "      (18): Conv1d(64, 64, kernel_size=(512,), stride=(2,), padding=(256,))\n",
      "      (19): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (20): ReLU()\n",
      "      (21): Dropout(p=0.3, inplace=False)\n",
      "      (22): AdaptiveAvgPool1d(output_size=1)\n",
      "    )\n",
      "    (fc): Linear(in_features=64, out_features=128, bias=True)\n",
      "  )\n",
      ")\n",
      "Total trainable parameters: 2,705,140\n",
      "\n",
      "Trainable parameters by layer:\n",
      "encoder.layers.1.weight                            64\n",
      "encoder.layers.1.bias                              4\n",
      "encoder.layers.2.weight                            4\n",
      "encoder.layers.2.bias                              4\n",
      "encoder.layers.5.weight                            1,024\n",
      "encoder.layers.5.bias                              8\n",
      "encoder.layers.6.weight                            8\n",
      "encoder.layers.6.bias                              8\n",
      "encoder.layers.9.weight                            8,192\n",
      "encoder.layers.9.bias                              16\n",
      "encoder.layers.10.weight                           16\n",
      "encoder.layers.10.bias                             16\n",
      "encoder.layers.12.weight                           65,536\n",
      "encoder.layers.12.bias                             32\n",
      "encoder.layers.13.weight                           32\n",
      "encoder.layers.13.bias                             32\n",
      "encoder.layers.15.weight                           524,288\n",
      "encoder.layers.15.bias                             64\n",
      "encoder.layers.16.weight                           64\n",
      "encoder.layers.16.bias                             64\n",
      "encoder.layers.18.weight                           2,097,152\n",
      "encoder.layers.18.bias                             64\n",
      "encoder.layers.19.weight                           64\n",
      "encoder.layers.19.bias                             64\n",
      "encoder.fc.weight                                  8,192\n",
      "encoder.fc.bias                                    128\n",
      "===== Training Input Shape Info =====\n",
      "Train input shapes: x1: torch.Size([256, 1, 24000]), x2: torch.Size([256, 1, 24000])\n",
      "===== Testing Input Shape Info =====\n",
      "Test input shapes: x1: torch.Size([256, 1, 24000]), x2: torch.Size([256, 1, 24000])\n",
      "Preds: tensor([0., 0., 1., 0., 0., 0., 1., 0., 0., 1.])\n",
      "Labels: tensor([0., 1., 0., 0., 0., 0., 1., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "if __name__ == \"__main__\":\n",
    "    model = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50439854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24414"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebd612b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd8e416",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae17a80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfab46f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d560822",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a670a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

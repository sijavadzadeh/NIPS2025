{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690b705c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# siamese_lfp_model.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "cf93163b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Config ------------\n",
    "SEGMENT_LENGTH = 24000  # 1 second @ 24kHz\n",
    "BATCH_SIZE = 256\n",
    "EMBEDDING_DIM = 128\n",
    "LEARNING_RATE = 1e-4\n",
    "EPOCHS = 200\n",
    "LOG_DIR = \"runs/siamese_lfp3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "ad389620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Data Preparation ------------\n",
    "def segment_data(data, segment_length):\n",
    "    num_segments = len(data) // segment_length\n",
    "    segments = np.array(np.split(data[:num_segments * segment_length], num_segments))\n",
    "    np.random.shuffle(segments)\n",
    "    return segments\n",
    "\n",
    "\n",
    "# def create_pairs(gpi_segments, stn_segments):\n",
    "#     pairs = []\n",
    "#     labels = []\n",
    "\n",
    "#     # GPi-GPi (label 0)\n",
    "#     for i in range(len(gpi_segments) - 1):\n",
    "#         pairs.append((gpi_segments[i], gpi_segments[i+1]))\n",
    "#         labels.append(0)\n",
    "\n",
    "#     # STN-STN (label 0)\n",
    "#     for i in range(len(stn_segments) - 1):\n",
    "#         pairs.append((stn_segments[i], stn_segments[i+1]))\n",
    "#         labels.append(0)\n",
    "\n",
    "#     # GPi-STN (label 1)\n",
    "#     for i in range(min(len(gpi_segments), len(stn_segments))):\n",
    "#         pairs.append((gpi_segments[i], stn_segments[i]))\n",
    "#         labels.append(1)\n",
    "\n",
    "#     ratio_similar = labels.count(0) / len(labels)\n",
    "#     print(f\"Similarity label ratio: {ratio_similar:.2f} similar (label 0), {1 - ratio_similar:.2f} dissimilar (label 1)\")\n",
    "\n",
    "#     return pairs, labels\n",
    "\n",
    "def create_pairs(gpi_segments, stn_segments):\n",
    "    # GPi-GPi (label 0)\n",
    "    gpi_gpi_pairs = [(gpi_segments[i], gpi_segments[i+1]) for i in range(len(gpi_segments) - 1)]\n",
    "    gpi_gpi_labels = [0] * len(gpi_gpi_pairs)\n",
    "\n",
    "    # STN-STN (label 0)\n",
    "    stn_stn_pairs = [(stn_segments[i], stn_segments[i+1]) for i in range(len(stn_segments) - 1)]\n",
    "    stn_stn_labels = [0] * len(stn_stn_pairs)\n",
    "\n",
    "    # GPi-STN (label 1)\n",
    "    gpi_stn_pairs = [(gpi_segments[i], stn_segments[i]) for i in range(min(len(gpi_segments), len(stn_segments)))]\n",
    "    gpi_stn_labels = [1] * len(gpi_stn_pairs)\n",
    "\n",
    "    # Combine similar pairs\n",
    "    similar_pairs = gpi_gpi_pairs + stn_stn_pairs\n",
    "    similar_labels = gpi_gpi_labels + stn_stn_labels\n",
    "\n",
    "    # Balance both classes\n",
    "    min_len = min(len(similar_pairs), len(gpi_stn_pairs))\n",
    "    balanced_pairs = similar_pairs[:min_len] + gpi_stn_pairs[:min_len]\n",
    "    balanced_labels = similar_labels[:min_len] + gpi_stn_labels[:min_len]\n",
    "\n",
    "    ratio_similar = balanced_labels.count(0) / len(balanced_labels)\n",
    "    print(f\"Balanced similarity label ratio: {ratio_similar:.2f} similar (label 0), {1 - ratio_similar:.2f} dissimilar (label 1)\")\n",
    "\n",
    "    return balanced_pairs, balanced_labels\n",
    "\n",
    "\n",
    "class LFPDataset(Dataset):\n",
    "    def __init__(self, pairs, labels):\n",
    "        self.pairs = pairs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x1, x2 = self.pairs[idx]\n",
    "        x1 = torch.tensor(x1, dtype=torch.float32).unsqueeze(0)\n",
    "        x2 = torch.tensor(x2, dtype=torch.float32).unsqueeze(0)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        return x1, x2, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "86b0c84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Model Definition ------------\n",
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNNEncoder, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Conv1d(1, 4, kernel_size=16, stride=2, padding=8),\n",
    "            nn.BatchNorm1d(4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Conv1d(4, 8, kernel_size=32, stride=2, padding=16),\n",
    "            nn.BatchNorm1d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Conv1d(8, 16, kernel_size=64, stride=2, padding=32),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(0.1),\n",
    "            nn.Conv1d(16, 32, kernel_size=128, stride=2, padding=64),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(0.1),\n",
    "            nn.Conv1d(32, 64, kernel_size=256, stride=2, padding=128),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(0.1),\n",
    "            nn.Conv1d(64, 64, kernel_size=512, stride=2, padding=256),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "        ])\n",
    "        self.fc = nn.Linear(64, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    total = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total trainable parameters: {total:,}\\n\")\n",
    "    print(\"Trainable parameters by layer:\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f\"{name:50} {param.numel():,}\")\n",
    "\n",
    "\n",
    "def print_model_summary(model, train_loader, device):\n",
    "    print(\"===== Sample Forward Pass Shape Info =====\")\n",
    "    x_sample = next(iter(train_loader))[0].to(device)\n",
    "    model.encoder(x_sample)\n",
    "    print(\"\\nModel Summary:\\n\")\n",
    "    print(model)\n",
    "    count_parameters(model)\n",
    "\n",
    "\n",
    "class SiameseNet(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(SiameseNet, self).__init__()\n",
    "        self.encoder = CNNEncoder(embedding_dim)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        embed1 = self.encoder(x1)\n",
    "        embed2 = self.encoder(x2)\n",
    "        distance = F.pairwise_distance(embed1, embed2)\n",
    "        return distance \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "c0ee84f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------- Training Loop ------------\n",
    "def train(model, dataloader, optimizer, criterion, device, writer, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (x1, x2, label) in enumerate(dataloader):\n",
    "        x1, x2, label = x1.to(device), x2.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x1, x2)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        preds = (output > 0.5).float()\n",
    "        correct += (preds == label).sum().item()\n",
    "        total += label.size(0)\n",
    "        if i == 0:\n",
    "            print(\"Preds:\", preds[:10])\n",
    "            print(\"Labels:\", label[:10])\n",
    "\n",
    "        writer.add_scalar('Train/Batch_Loss', loss.item(), epoch * len(dataloader) + i)\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    accuracy = correct / total\n",
    "    print(f\"Epoch {epoch+1} Train Accuracy: {accuracy:.4f}\")\n",
    "    writer.add_scalar('Train/Epoch_Loss', avg_loss, epoch)\n",
    "    writer.add_scalar('Train/Accuracy', accuracy, epoch)\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device, writer, epoch):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x1, x2, label in dataloader:\n",
    "            x1, x2, label = x1.to(device), x2.to(device), label.to(device)\n",
    "            output = model(x1, x2)\n",
    "            loss = criterion(output, label)\n",
    "            total_loss += loss.item()\n",
    "            preds = (output > 0.5).float()  # Same for both train and test\n",
    "            correct += (preds == label).sum().item()\n",
    "            total += label.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct / total\n",
    "    print(f\"Epoch {epoch+1} Test Accuracy: {accuracy:.4f}\")\n",
    "    writer.add_scalar('Test/Epoch_Loss', avg_loss, epoch)\n",
    "    writer.add_scalar('Test/Accuracy', accuracy, epoch)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57497af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Run Pipeline ------------\n",
    "def main():\n",
    "    # Print dataset shape and sizes\n",
    "    print(\"===== Dataset Information =====\")\n",
    "    # TensorBoard setup\n",
    "    os.makedirs(LOG_DIR, exist_ok=True)\n",
    "    writer = SummaryWriter(LOG_DIR)\n",
    "\n",
    "    # # Simulate random LFP-like signals\n",
    "    # GPiData = np.random.randn(24000 * 60 * 2)  # 2 minutes of data\n",
    "    # STNData = np.random.randn(24000 * 60 * 2)\n",
    "    # Simulate random LFP-like signals\n",
    "    # Generate synthetic LFP-like signals with sinusoids + noise\n",
    "\n",
    "    # fs = 24000\n",
    "    # duration_sec = 60 * 2\n",
    "    # t = np.linspace(0, duration_sec, fs * duration_sec, endpoint=False)\n",
    "\n",
    "    # gpi_signal = np.sin(2 * np.pi * 10 * t)  # 10 Hz sinusoid\n",
    "    # stn_signal = np.sin(2 * np.pi * 10 * t)  # 30 Hz sinusoid\n",
    "\n",
    "    # GPiData = gpi_signal + 0.5 * np.random.randn(len(t))\n",
    "    # STNData = stn_signal + 0.5 * np.random.randn(len(t))\n",
    "\n",
    "    with h5py.File(\"F:\\Python Projects\\data\\period9\\microGPi1_L_1_CommonFiltered.mat\", \"r\") as f:\n",
    "        GPiData = np.array(f[\"data\"]).squeeze()\n",
    "        fs = int(np.array(f[\"fs\"]).squeeze())\n",
    "        # rtesting\n",
    "\n",
    "# G:\\Coincidence\\Data\\s514_202301\\Mat Data\\Voluntary\\micro_CommonFiltered\\period1\\microVoSTN_L_3_CommonFiltered.mat\"\n",
    "    with h5py.File(\"F:\\Python Projects\\data\\period9\\microSTN_L_3_CommonFiltered.mat\", \"r\") as f:\n",
    "        STNData = np.array(f[\"data\"]).squeeze()  # replace with your actual filename\n",
    "\n",
    "    # Segment\n",
    "    gpi_segments = segment_data(GPiData, SEGMENT_LENGTH)\n",
    "    stn_segments = segment_data(STNData, SEGMENT_LENGTH)\n",
    "    pairs, labels = create_pairs(gpi_segments, stn_segments)\n",
    "\n",
    "    # Split data\n",
    "    # train_pairs, test_pairs, train_labels, test_labels = train_test_split(pairs, labels, test_size=0.2, stratify=labels)\n",
    "    train_pairs, test_pairs, train_labels, test_labels = train_test_split( pairs, labels, test_size=0.2, stratify=labels, shuffle=True, random_state=42)\n",
    "\n",
    "    # Dataloaders\n",
    "    train_ds = LFPDataset(train_pairs, train_labels)\n",
    "    test_ds = LFPDataset(test_pairs, test_labels)\n",
    "    print(f\"Train set: {len(train_ds)} pairs\")\n",
    "    print(f\"Test set: {len(test_ds)} pairs\")\n",
    "    print(f\"Batch size: {BATCH_SIZE}\")\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE)\n",
    "\n",
    "    # Model setup\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = SiameseNet(embedding_dim=EMBEDDING_DIM).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    print_model_summary(model, train_loader, device)\n",
    "    print(\"===== Training Input Shape Info =====\")\n",
    "    for x1, x2, _ in train_loader:\n",
    "        print(f\"Train input shapes: x1: {x1.shape}, x2: {x2.shape}\")\n",
    "        break\n",
    "    print(\"===== Testing Input Shape Info =====\")\n",
    "\n",
    "    def contrastive_loss(distances, labels, margin=1.0):\n",
    "        labels = labels.float()\n",
    "        loss_similar = (1 - labels) * distances.pow(2)\n",
    "        loss_dissimilar = labels * F.relu(margin - distances).pow(2)\n",
    "        return torch.mean(loss_similar + loss_dissimilar)\n",
    "\n",
    "    criterion = contrastive_loss\n",
    "    for x1, x2, _ in test_loader:\n",
    "        print(f\"Test input shapes: x1: {x1.shape}, x2: {x2.shape}\")\n",
    "        break\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "\n",
    "    \n",
    "        train_loss = train(model, train_loader, optimizer, criterion, device, writer, epoch)\n",
    "        test_loss = evaluate(model, test_loader, criterion, device, writer, epoch)\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    writer.close()\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "80f4d671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Dataset Information =====\n",
      "Balanced similarity label ratio: 0.50 similar (label 0), 0.50 dissimilar (label 1)\n",
      "Train set: 6177 pairs\n",
      "Test set: 1545 pairs\n",
      "Batch size: 256\n",
      "===== Sample Forward Pass Shape Info =====\n",
      "\n",
      "Model Summary:\n",
      "\n",
      "SiameseNet(\n",
      "  (encoder): CNNEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): Dropout(p=0.3, inplace=False)\n",
      "      (1): Conv1d(1, 4, kernel_size=(16,), stride=(2,), padding=(8,))\n",
      "      (2): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ReLU()\n",
      "      (4): Dropout(p=0.3, inplace=False)\n",
      "      (5): Conv1d(4, 8, kernel_size=(32,), stride=(2,), padding=(16,))\n",
      "      (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (7): ReLU()\n",
      "      (8): Dropout(p=0.3, inplace=False)\n",
      "      (9): Conv1d(8, 16, kernel_size=(64,), stride=(2,), padding=(32,))\n",
      "      (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (11): ReLU()\n",
      "      (12): Conv1d(16, 32, kernel_size=(128,), stride=(2,), padding=(64,))\n",
      "      (13): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (14): ReLU()\n",
      "      (15): Conv1d(32, 64, kernel_size=(256,), stride=(2,), padding=(128,))\n",
      "      (16): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (17): ReLU()\n",
      "      (18): Conv1d(64, 64, kernel_size=(512,), stride=(2,), padding=(256,))\n",
      "      (19): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (20): ReLU()\n",
      "      (21): Dropout(p=0.3, inplace=False)\n",
      "      (22): AdaptiveAvgPool1d(output_size=1)\n",
      "    )\n",
      "    (fc): Linear(in_features=64, out_features=128, bias=True)\n",
      "  )\n",
      ")\n",
      "Total trainable parameters: 2,705,140\n",
      "\n",
      "Trainable parameters by layer:\n",
      "encoder.layers.1.weight                            64\n",
      "encoder.layers.1.bias                              4\n",
      "encoder.layers.2.weight                            4\n",
      "encoder.layers.2.bias                              4\n",
      "encoder.layers.5.weight                            1,024\n",
      "encoder.layers.5.bias                              8\n",
      "encoder.layers.6.weight                            8\n",
      "encoder.layers.6.bias                              8\n",
      "encoder.layers.9.weight                            8,192\n",
      "encoder.layers.9.bias                              16\n",
      "encoder.layers.10.weight                           16\n",
      "encoder.layers.10.bias                             16\n",
      "encoder.layers.12.weight                           65,536\n",
      "encoder.layers.12.bias                             32\n",
      "encoder.layers.13.weight                           32\n",
      "encoder.layers.13.bias                             32\n",
      "encoder.layers.15.weight                           524,288\n",
      "encoder.layers.15.bias                             64\n",
      "encoder.layers.16.weight                           64\n",
      "encoder.layers.16.bias                             64\n",
      "encoder.layers.18.weight                           2,097,152\n",
      "encoder.layers.18.bias                             64\n",
      "encoder.layers.19.weight                           64\n",
      "encoder.layers.19.bias                             64\n",
      "encoder.fc.weight                                  8,192\n",
      "encoder.fc.bias                                    128\n",
      "===== Training Input Shape Info =====\n",
      "Train input shapes: x1: torch.Size([256, 1, 24000]), x2: torch.Size([256, 1, 24000])\n",
      "===== Testing Input Shape Info =====\n",
      "Test input shapes: x1: torch.Size([256, 1, 24000]), x2: torch.Size([256, 1, 24000])\n",
      "Preds: tensor([0., 0., 1., 0., 0., 0., 1., 0., 0., 1.])\n",
      "Labels: tensor([0., 1., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "Epoch 1 Train Accuracy: 0.5169\n",
      "Epoch 1 Test Accuracy: 0.5003\n",
      "Epoch 1/200, Train Loss: 0.8465, Test Loss: 0.4699\n",
      "Preds: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "Labels: tensor([1., 0., 0., 0., 1., 1., 0., 1., 1., 1.])\n",
      "Epoch 2 Train Accuracy: 0.5079\n",
      "Epoch 2 Test Accuracy: 0.5359\n",
      "Epoch 2/200, Train Loss: 0.3165, Test Loss: 0.3597\n",
      "Preds: tensor([0., 1., 0., 0., 0., 0., 0., 1., 1., 0.])\n",
      "Labels: tensor([1., 0., 1., 1., 1., 1., 1., 0., 0., 0.])\n",
      "Epoch 3 Train Accuracy: 0.5041\n",
      "Epoch 3 Test Accuracy: 0.5294\n",
      "Epoch 3/200, Train Loss: 0.3166, Test Loss: 0.3793\n",
      "Preds: tensor([1., 0., 0., 1., 0., 0., 0., 0., 0., 1.])\n",
      "Labels: tensor([1., 1., 0., 0., 0., 1., 0., 1., 1., 1.])\n",
      "Epoch 4 Train Accuracy: 0.5041\n",
      "Epoch 4 Test Accuracy: 0.5294\n",
      "Epoch 4/200, Train Loss: 0.2975, Test Loss: 0.3157\n",
      "Preds: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Labels: tensor([0., 1., 1., 0., 0., 0., 1., 0., 0., 1.])\n",
      "Epoch 5 Train Accuracy: 0.5066\n",
      "Epoch 5 Test Accuracy: 0.5333\n",
      "Epoch 5/200, Train Loss: 0.2978, Test Loss: 0.3032\n",
      "Preds: tensor([0., 0., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Labels: tensor([0., 1., 1., 1., 1., 0., 1., 0., 1., 0.])\n",
      "Epoch 6 Train Accuracy: 0.5185\n",
      "Epoch 6 Test Accuracy: 0.5307\n",
      "Epoch 6/200, Train Loss: 0.3064, Test Loss: 0.3153\n",
      "Preds: tensor([1., 0., 0., 0., 0., 1., 0., 1., 0., 0.])\n",
      "Labels: tensor([0., 1., 0., 0., 1., 1., 0., 0., 1., 0.])\n",
      "Epoch 7 Train Accuracy: 0.5082\n",
      "Epoch 7 Test Accuracy: 0.5359\n",
      "Epoch 7/200, Train Loss: 0.2996, Test Loss: 0.2989\n",
      "Preds: tensor([0., 0., 1., 1., 1., 1., 1., 0., 1., 1.])\n",
      "Labels: tensor([1., 1., 1., 0., 1., 0., 1., 0., 1., 1.])\n",
      "Epoch 8 Train Accuracy: 0.5082\n",
      "Epoch 8 Test Accuracy: 0.5049\n",
      "Epoch 8/200, Train Loss: 0.2867, Test Loss: 0.3189\n",
      "Preds: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "Labels: tensor([1., 1., 1., 1., 0., 1., 1., 1., 1., 0.])\n",
      "Epoch 9 Train Accuracy: 0.4989\n",
      "Epoch 9 Test Accuracy: 0.5307\n",
      "Epoch 9/200, Train Loss: 0.2887, Test Loss: 0.4095\n",
      "Preds: tensor([0., 1., 0., 1., 0., 0., 0., 0., 0., 1.])\n",
      "Labels: tensor([1., 0., 1., 1., 0., 0., 0., 1., 1., 0.])\n",
      "Epoch 10 Train Accuracy: 0.5145\n",
      "Epoch 10 Test Accuracy: 0.5405\n",
      "Epoch 10/200, Train Loss: 0.2851, Test Loss: 0.2884\n",
      "Preds: tensor([1., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "Labels: tensor([0., 1., 1., 1., 0., 0., 0., 0., 0., 0.])\n",
      "Epoch 11 Train Accuracy: 0.5049\n",
      "Epoch 11 Test Accuracy: 0.5417\n",
      "Epoch 11/200, Train Loss: 0.2954, Test Loss: 0.5095\n",
      "Preds: tensor([0., 0., 0., 0., 0., 0., 1., 1., 0., 0.])\n",
      "Labels: tensor([1., 1., 0., 1., 1., 0., 0., 0., 0., 1.])\n",
      "Epoch 12 Train Accuracy: 0.5148\n",
      "Epoch 12 Test Accuracy: 0.5327\n",
      "Epoch 12/200, Train Loss: 0.2759, Test Loss: 0.2972\n",
      "Preds: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "Labels: tensor([1., 1., 1., 1., 0., 0., 1., 1., 0., 1.])\n",
      "Epoch 13 Train Accuracy: 0.5150\n",
      "Epoch 13 Test Accuracy: 0.5217\n",
      "Epoch 13/200, Train Loss: 0.2705, Test Loss: 0.2858\n",
      "Preds: tensor([1., 0., 0., 0., 1., 0., 0., 1., 1., 0.])\n",
      "Labels: tensor([1., 1., 1., 0., 1., 1., 1., 0., 0., 0.])\n",
      "Epoch 14 Train Accuracy: 0.5169\n",
      "Epoch 14 Test Accuracy: 0.5282\n",
      "Epoch 14/200, Train Loss: 0.2710, Test Loss: 0.3159\n",
      "Preds: tensor([0., 1., 1., 1., 0., 1., 0., 1., 0., 1.])\n",
      "Labels: tensor([1., 0., 0., 0., 1., 1., 1., 1., 0., 0.])\n",
      "Epoch 15 Train Accuracy: 0.5145\n",
      "Epoch 15 Test Accuracy: 0.5236\n",
      "Epoch 15/200, Train Loss: 0.2781, Test Loss: 0.2730\n",
      "Preds: tensor([1., 0., 1., 1., 1., 1., 0., 0., 1., 1.])\n",
      "Labels: tensor([1., 0., 1., 1., 1., 0., 0., 1., 0., 0.])\n",
      "Epoch 16 Train Accuracy: 0.5083\n",
      "Epoch 16 Test Accuracy: 0.5087\n",
      "Epoch 16/200, Train Loss: 0.2714, Test Loss: 0.2897\n",
      "Preds: tensor([0., 0., 1., 1., 0., 1., 1., 0., 1., 1.])\n",
      "Labels: tensor([1., 0., 1., 0., 0., 0., 1., 1., 1., 0.])\n",
      "Epoch 17 Train Accuracy: 0.5247\n",
      "Epoch 17 Test Accuracy: 0.5405\n",
      "Epoch 17/200, Train Loss: 0.2632, Test Loss: 0.2718\n",
      "Preds: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "Labels: tensor([1., 1., 1., 1., 1., 1., 0., 0., 1., 0.])\n",
      "Epoch 18 Train Accuracy: 0.5202\n",
      "Epoch 18 Test Accuracy: 0.5327\n",
      "Epoch 18/200, Train Loss: 0.2672, Test Loss: 0.2553\n",
      "Preds: tensor([0., 0., 1., 0., 0., 0., 1., 1., 0., 1.])\n",
      "Labels: tensor([0., 1., 1., 1., 0., 1., 0., 0., 1., 0.])\n",
      "Epoch 19 Train Accuracy: 0.5205\n",
      "Epoch 19 Test Accuracy: 0.5424\n",
      "Epoch 19/200, Train Loss: 0.2647, Test Loss: 0.2879\n",
      "Preds: tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 0.])\n",
      "Labels: tensor([0., 0., 0., 1., 1., 1., 0., 0., 1., 0.])\n",
      "Epoch 20 Train Accuracy: 0.5177\n",
      "Epoch 20 Test Accuracy: 0.5236\n",
      "Epoch 20/200, Train Loss: 0.2638, Test Loss: 0.2527\n",
      "Preds: tensor([1., 0., 1., 1., 1., 1., 0., 0., 0., 0.])\n",
      "Labels: tensor([0., 1., 1., 1., 1., 0., 0., 0., 0., 1.])\n",
      "Epoch 21 Train Accuracy: 0.5255\n",
      "Epoch 21 Test Accuracy: 0.5463\n",
      "Epoch 21/200, Train Loss: 0.2609, Test Loss: 0.3475\n",
      "Preds: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Labels: tensor([1., 1., 0., 0., 1., 0., 1., 0., 1., 1.])\n",
      "Epoch 22 Train Accuracy: 0.5318\n",
      "Epoch 22 Test Accuracy: 0.5366\n",
      "Epoch 22/200, Train Loss: 0.2608, Test Loss: 0.3070\n",
      "Preds: tensor([1., 0., 1., 0., 0., 0., 0., 0., 1., 0.])\n",
      "Labels: tensor([1., 0., 0., 0., 0., 1., 0., 0., 0., 1.])\n",
      "Epoch 23 Train Accuracy: 0.5279\n",
      "Epoch 23 Test Accuracy: 0.5061\n",
      "Epoch 23/200, Train Loss: 0.2590, Test Loss: 0.2589\n",
      "Preds: tensor([1., 0., 0., 0., 1., 1., 0., 1., 1., 0.])\n",
      "Labels: tensor([1., 1., 0., 1., 1., 0., 0., 0., 0., 0.])\n",
      "Epoch 24 Train Accuracy: 0.5224\n",
      "Epoch 24 Test Accuracy: 0.5346\n",
      "Epoch 24/200, Train Loss: 0.2661, Test Loss: 0.4857\n",
      "Preds: tensor([0., 0., 1., 0., 0., 0., 0., 0., 1., 0.])\n",
      "Labels: tensor([0., 0., 0., 1., 0., 0., 1., 0., 1., 1.])\n",
      "Epoch 25 Train Accuracy: 0.5329\n",
      "Epoch 25 Test Accuracy: 0.5495\n",
      "Epoch 25/200, Train Loss: 0.2561, Test Loss: 0.2771\n",
      "Preds: tensor([0., 1., 1., 1., 0., 0., 0., 0., 0., 1.])\n",
      "Labels: tensor([1., 1., 1., 0., 1., 1., 1., 0., 0., 1.])\n",
      "Epoch 26 Train Accuracy: 0.5372\n",
      "Epoch 26 Test Accuracy: 0.5411\n",
      "Epoch 26/200, Train Loss: 0.2595, Test Loss: 0.2518\n",
      "Preds: tensor([0., 1., 0., 1., 0., 1., 0., 0., 0., 1.])\n",
      "Labels: tensor([0., 1., 1., 1., 0., 1., 0., 1., 1., 0.])\n",
      "Epoch 27 Train Accuracy: 0.5359\n",
      "Epoch 27 Test Accuracy: 0.5081\n",
      "Epoch 27/200, Train Loss: 0.2583, Test Loss: 0.2851\n",
      "Preds: tensor([0., 1., 1., 0., 1., 0., 1., 0., 1., 0.])\n",
      "Labels: tensor([1., 1., 1., 1., 1., 0., 0., 1., 0., 1.])\n",
      "Epoch 28 Train Accuracy: 0.5310\n",
      "Epoch 28 Test Accuracy: 0.4932\n",
      "Epoch 28/200, Train Loss: 0.2587, Test Loss: 0.3446\n",
      "Preds: tensor([0., 0., 1., 1., 0., 1., 0., 0., 0., 1.])\n",
      "Labels: tensor([1., 1., 0., 1., 0., 0., 0., 0., 1., 1.])\n",
      "Epoch 29 Train Accuracy: 0.5394\n",
      "Epoch 29 Test Accuracy: 0.5139\n",
      "Epoch 29/200, Train Loss: 0.2570, Test Loss: 0.2647\n",
      "Preds: tensor([1., 1., 0., 0., 1., 0., 1., 0., 1., 1.])\n",
      "Labels: tensor([0., 0., 0., 1., 1., 0., 1., 0., 0., 0.])\n",
      "Epoch 30 Train Accuracy: 0.5461\n",
      "Epoch 30 Test Accuracy: 0.5366\n",
      "Epoch 30/200, Train Loss: 0.2566, Test Loss: 0.3115\n",
      "Preds: tensor([1., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "Labels: tensor([1., 0., 1., 1., 1., 1., 0., 0., 1., 1.])\n",
      "Epoch 31 Train Accuracy: 0.5440\n",
      "Epoch 31 Test Accuracy: 0.5502\n",
      "Epoch 31/200, Train Loss: 0.2528, Test Loss: 0.2884\n",
      "Preds: tensor([1., 1., 1., 1., 0., 0., 1., 1., 1., 0.])\n",
      "Labels: tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 1.])\n",
      "Epoch 32 Train Accuracy: 0.5414\n",
      "Epoch 32 Test Accuracy: 0.5456\n",
      "Epoch 32/200, Train Loss: 0.2560, Test Loss: 0.3667\n",
      "Preds: tensor([0., 0., 1., 0., 0., 0., 0., 0., 1., 1.])\n",
      "Labels: tensor([0., 1., 0., 1., 0., 1., 1., 0., 0., 1.])\n",
      "Epoch 33 Train Accuracy: 0.5454\n",
      "Epoch 33 Test Accuracy: 0.5314\n",
      "Epoch 33/200, Train Loss: 0.2531, Test Loss: 0.2801\n",
      "Preds: tensor([0., 1., 0., 0., 0., 0., 1., 1., 0., 0.])\n",
      "Labels: tensor([0., 0., 0., 0., 0., 0., 1., 1., 0., 0.])\n",
      "Epoch 34 Train Accuracy: 0.5414\n",
      "Epoch 34 Test Accuracy: 0.5049\n",
      "Epoch 34/200, Train Loss: 0.2518, Test Loss: 0.2738\n",
      "Preds: tensor([0., 0., 1., 0., 1., 1., 1., 1., 0., 0.])\n",
      "Labels: tensor([0., 1., 0., 0., 0., 1., 1., 0., 0., 1.])\n",
      "Epoch 35 Train Accuracy: 0.5495\n",
      "Epoch 35 Test Accuracy: 0.5579\n",
      "Epoch 35/200, Train Loss: 0.2557, Test Loss: 0.4049\n",
      "Preds: tensor([1., 0., 1., 1., 0., 0., 0., 0., 1., 1.])\n",
      "Labels: tensor([1., 1., 1., 0., 0., 0., 0., 0., 1., 1.])\n",
      "Epoch 36 Train Accuracy: 0.5475\n",
      "Epoch 36 Test Accuracy: 0.5282\n",
      "Epoch 36/200, Train Loss: 0.2520, Test Loss: 0.3758\n",
      "Preds: tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])\n",
      "Labels: tensor([1., 0., 1., 1., 0., 0., 1., 1., 0., 0.])\n",
      "Epoch 37 Train Accuracy: 0.5554\n",
      "Epoch 37 Test Accuracy: 0.5074\n",
      "Epoch 37/200, Train Loss: 0.2497, Test Loss: 0.2680\n",
      "Preds: tensor([0., 0., 0., 0., 0., 1., 0., 1., 0., 0.])\n",
      "Labels: tensor([1., 0., 0., 0., 1., 1., 0., 0., 0., 1.])\n",
      "Epoch 38 Train Accuracy: 0.5427\n",
      "Epoch 38 Test Accuracy: 0.5197\n",
      "Epoch 38/200, Train Loss: 0.2578, Test Loss: 0.2620\n",
      "Preds: tensor([0., 1., 1., 0., 0., 1., 1., 0., 0., 1.])\n",
      "Labels: tensor([1., 0., 1., 1., 1., 1., 1., 0., 0., 0.])\n",
      "Epoch 39 Train Accuracy: 0.5454\n",
      "Epoch 39 Test Accuracy: 0.5165\n",
      "Epoch 39/200, Train Loss: 0.2510, Test Loss: 0.2651\n",
      "Preds: tensor([0., 0., 0., 1., 0., 1., 1., 0., 1., 0.])\n",
      "Labels: tensor([1., 1., 1., 1., 0., 1., 1., 0., 0., 1.])\n",
      "Epoch 40 Train Accuracy: 0.5587\n",
      "Epoch 40 Test Accuracy: 0.4984\n",
      "Epoch 40/200, Train Loss: 0.2486, Test Loss: 0.3302\n",
      "Preds: tensor([0., 1., 0., 0., 1., 1., 0., 0., 0., 1.])\n",
      "Labels: tensor([1., 0., 0., 1., 0., 0., 0., 0., 1., 0.])\n",
      "Epoch 41 Train Accuracy: 0.5656\n",
      "Epoch 41 Test Accuracy: 0.5353\n",
      "Epoch 41/200, Train Loss: 0.2470, Test Loss: 0.2488\n",
      "Preds: tensor([1., 0., 1., 1., 0., 0., 0., 0., 0., 0.])\n",
      "Labels: tensor([1., 1., 1., 1., 1., 0., 0., 0., 1., 0.])\n",
      "Epoch 42 Train Accuracy: 0.5676\n",
      "Epoch 42 Test Accuracy: 0.5081\n",
      "Epoch 42/200, Train Loss: 0.2446, Test Loss: 0.2630\n",
      "Preds: tensor([1., 0., 0., 1., 0., 1., 1., 0., 1., 1.])\n",
      "Labels: tensor([1., 0., 0., 1., 0., 1., 0., 0., 0., 0.])\n",
      "Epoch 43 Train Accuracy: 0.5554\n",
      "Epoch 43 Test Accuracy: 0.5566\n",
      "Epoch 43/200, Train Loss: 0.2500, Test Loss: 0.3603\n",
      "Preds: tensor([0., 0., 0., 1., 1., 0., 0., 0., 1., 0.])\n",
      "Labels: tensor([0., 0., 0., 0., 1., 0., 0., 1., 0., 1.])\n",
      "Epoch 44 Train Accuracy: 0.5634\n",
      "Epoch 44 Test Accuracy: 0.5443\n",
      "Epoch 44/200, Train Loss: 0.2478, Test Loss: 0.3348\n",
      "Preds: tensor([1., 0., 1., 0., 1., 1., 0., 0., 1., 1.])\n",
      "Labels: tensor([0., 1., 1., 0., 0., 1., 1., 1., 1., 1.])\n",
      "Epoch 45 Train Accuracy: 0.5574\n",
      "Epoch 45 Test Accuracy: 0.5178\n",
      "Epoch 45/200, Train Loss: 0.2467, Test Loss: 0.2696\n",
      "Preds: tensor([1., 1., 1., 0., 1., 0., 1., 0., 0., 1.])\n",
      "Labels: tensor([1., 0., 1., 0., 0., 0., 1., 1., 0., 1.])\n",
      "Epoch 46 Train Accuracy: 0.5695\n",
      "Epoch 46 Test Accuracy: 0.5553\n",
      "Epoch 46/200, Train Loss: 0.2439, Test Loss: 0.2772\n",
      "Preds: tensor([1., 0., 1., 1., 0., 0., 0., 0., 1., 0.])\n",
      "Labels: tensor([0., 1., 0., 0., 0., 0., 0., 1., 1., 1.])\n",
      "Epoch 47 Train Accuracy: 0.5930\n",
      "Epoch 47 Test Accuracy: 0.5003\n",
      "Epoch 47/200, Train Loss: 0.2392, Test Loss: 0.4197\n",
      "Preds: tensor([1., 0., 0., 1., 0., 1., 1., 0., 1., 0.])\n",
      "Labels: tensor([0., 0., 0., 1., 0., 1., 1., 1., 1., 1.])\n",
      "Epoch 48 Train Accuracy: 0.5949\n",
      "Epoch 48 Test Accuracy: 0.5482\n",
      "Epoch 48/200, Train Loss: 0.2384, Test Loss: 0.2842\n",
      "Preds: tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 0.])\n",
      "Labels: tensor([0., 1., 0., 1., 1., 0., 1., 0., 0., 1.])\n",
      "Epoch 49 Train Accuracy: 0.5899\n",
      "Epoch 49 Test Accuracy: 0.5359\n",
      "Epoch 49/200, Train Loss: 0.2397, Test Loss: 0.2706\n",
      "Preds: tensor([0., 0., 0., 1., 0., 0., 0., 1., 1., 0.])\n",
      "Labels: tensor([0., 1., 1., 1., 0., 0., 0., 1., 1., 0.])\n",
      "Epoch 50 Train Accuracy: 0.6016\n",
      "Epoch 50 Test Accuracy: 0.5003\n",
      "Epoch 50/200, Train Loss: 0.2340, Test Loss: 0.4745\n",
      "Preds: tensor([1., 1., 1., 1., 1., 1., 0., 0., 0., 0.])\n",
      "Labels: tensor([0., 0., 1., 1., 0., 0., 1., 1., 1., 1.])\n",
      "Epoch 51 Train Accuracy: 0.6014\n",
      "Epoch 51 Test Accuracy: 0.5184\n",
      "Epoch 51/200, Train Loss: 0.2379, Test Loss: 0.3799\n",
      "Preds: tensor([0., 1., 1., 1., 0., 1., 0., 1., 1., 1.])\n",
      "Labels: tensor([0., 1., 1., 1., 0., 1., 1., 1., 0., 1.])\n",
      "Epoch 52 Train Accuracy: 0.6293\n",
      "Epoch 52 Test Accuracy: 0.5003\n",
      "Epoch 52/200, Train Loss: 0.2291, Test Loss: 0.4152\n",
      "Preds: tensor([0., 0., 0., 1., 0., 0., 1., 1., 1., 0.])\n",
      "Labels: tensor([1., 1., 1., 0., 1., 0., 0., 0., 0., 1.])\n",
      "Epoch 53 Train Accuracy: 0.6243\n",
      "Epoch 53 Test Accuracy: 0.5055\n",
      "Epoch 53/200, Train Loss: 0.2329, Test Loss: 0.4428\n",
      "Preds: tensor([0., 0., 0., 1., 0., 0., 0., 1., 0., 0.])\n",
      "Labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Epoch 54 Train Accuracy: 0.6247\n",
      "Epoch 54 Test Accuracy: 0.5120\n",
      "Epoch 54/200, Train Loss: 0.2303, Test Loss: 0.4098\n",
      "Preds: tensor([1., 0., 0., 0., 0., 1., 0., 1., 0., 0.])\n",
      "Labels: tensor([0., 0., 0., 0., 0., 1., 1., 1., 0., 0.])\n",
      "Epoch 55 Train Accuracy: 0.6249\n",
      "Epoch 55 Test Accuracy: 0.4971\n",
      "Epoch 55/200, Train Loss: 0.2250, Test Loss: 0.3439\n",
      "Preds: tensor([0., 0., 1., 0., 1., 0., 0., 1., 1., 0.])\n",
      "Labels: tensor([1., 0., 1., 1., 0., 0., 0., 1., 1., 1.])\n",
      "Epoch 56 Train Accuracy: 0.6686\n",
      "Epoch 56 Test Accuracy: 0.5003\n",
      "Epoch 56/200, Train Loss: 0.2155, Test Loss: 0.4609\n",
      "Preds: tensor([1., 0., 1., 0., 0., 0., 0., 0., 1., 1.])\n",
      "Labels: tensor([1., 1., 1., 1., 1., 0., 0., 1., 1., 1.])\n",
      "Epoch 57 Train Accuracy: 0.6212\n",
      "Epoch 57 Test Accuracy: 0.4997\n",
      "Epoch 57/200, Train Loss: 0.2305, Test Loss: 0.3665\n",
      "Preds: tensor([0., 1., 0., 1., 0., 0., 1., 1., 0., 1.])\n",
      "Labels: tensor([0., 1., 1., 1., 1., 0., 1., 1., 0., 0.])\n",
      "Epoch 58 Train Accuracy: 0.6963\n",
      "Epoch 58 Test Accuracy: 0.5087\n",
      "Epoch 58/200, Train Loss: 0.2022, Test Loss: 0.4443\n",
      "Preds: tensor([1., 1., 1., 0., 1., 0., 1., 0., 0., 0.])\n",
      "Labels: tensor([1., 0., 0., 0., 1., 1., 1., 0., 1., 0.])\n",
      "Epoch 59 Train Accuracy: 0.6971\n",
      "Epoch 59 Test Accuracy: 0.5320\n",
      "Epoch 59/200, Train Loss: 0.2032, Test Loss: 0.4142\n",
      "Preds: tensor([1., 0., 0., 1., 1., 0., 1., 0., 1., 0.])\n",
      "Labels: tensor([1., 0., 0., 0., 1., 1., 0., 0., 0., 0.])\n",
      "Epoch 60 Train Accuracy: 0.7033\n",
      "Epoch 60 Test Accuracy: 0.5372\n",
      "Epoch 60/200, Train Loss: 0.2133, Test Loss: 0.2791\n",
      "Preds: tensor([1., 0., 0., 1., 0., 1., 0., 1., 0., 1.])\n",
      "Labels: tensor([1., 0., 0., 0., 0., 1., 0., 0., 1., 1.])\n",
      "Epoch 61 Train Accuracy: 0.6686\n",
      "Epoch 61 Test Accuracy: 0.5055\n",
      "Epoch 61/200, Train Loss: 0.2126, Test Loss: 0.7711\n",
      "Preds: tensor([0., 0., 1., 1., 1., 1., 1., 1., 0., 1.])\n",
      "Labels: tensor([0., 0., 0., 1., 0., 1., 1., 1., 0., 0.])\n",
      "Epoch 62 Train Accuracy: 0.7123\n",
      "Epoch 62 Test Accuracy: 0.5049\n",
      "Epoch 62/200, Train Loss: 0.1956, Test Loss: 0.3439\n",
      "Preds: tensor([1., 1., 1., 0., 1., 1., 0., 1., 1., 1.])\n",
      "Labels: tensor([1., 1., 1., 0., 0., 1., 0., 1., 1., 0.])\n",
      "Epoch 63 Train Accuracy: 0.7243\n",
      "Epoch 63 Test Accuracy: 0.5003\n",
      "Epoch 63/200, Train Loss: 0.1960, Test Loss: 0.4516\n",
      "Preds: tensor([1., 0., 1., 0., 0., 1., 1., 1., 0., 1.])\n",
      "Labels: tensor([0., 0., 1., 0., 0., 0., 1., 0., 1., 0.])\n",
      "Epoch 64 Train Accuracy: 0.7160\n",
      "Epoch 64 Test Accuracy: 0.5301\n",
      "Epoch 64/200, Train Loss: 0.1986, Test Loss: 0.3247\n",
      "Preds: tensor([1., 0., 1., 1., 0., 1., 1., 0., 0., 0.])\n",
      "Labels: tensor([1., 1., 1., 1., 0., 0., 0., 0., 0., 0.])\n",
      "Epoch 65 Train Accuracy: 0.7293\n",
      "Epoch 65 Test Accuracy: 0.5087\n",
      "Epoch 65/200, Train Loss: 0.1873, Test Loss: 0.3948\n",
      "Preds: tensor([0., 1., 0., 1., 0., 0., 0., 1., 0., 1.])\n",
      "Labels: tensor([0., 1., 0., 1., 1., 1., 1., 1., 0., 1.])\n",
      "Epoch 66 Train Accuracy: 0.7358\n",
      "Epoch 66 Test Accuracy: 0.5346\n",
      "Epoch 66/200, Train Loss: 0.1852, Test Loss: 0.4075\n",
      "Preds: tensor([1., 1., 0., 0., 0., 0., 1., 0., 1., 1.])\n",
      "Labels: tensor([1., 1., 0., 0., 0., 0., 1., 0., 1., 1.])\n",
      "Epoch 67 Train Accuracy: 0.7476\n",
      "Epoch 67 Test Accuracy: 0.5003\n",
      "Epoch 67/200, Train Loss: 0.1955, Test Loss: 0.4680\n",
      "Preds: tensor([1., 0., 0., 1., 1., 0., 1., 1., 0., 1.])\n",
      "Labels: tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 0.])\n",
      "Epoch 68 Train Accuracy: 0.7164\n",
      "Epoch 68 Test Accuracy: 0.5256\n",
      "Epoch 68/200, Train Loss: 0.1956, Test Loss: 0.4023\n",
      "Preds: tensor([1., 0., 1., 1., 0., 0., 0., 0., 1., 1.])\n",
      "Labels: tensor([1., 0., 1., 0., 1., 1., 0., 0., 0., 1.])\n",
      "Epoch 69 Train Accuracy: 0.7363\n",
      "Epoch 69 Test Accuracy: 0.5061\n",
      "Epoch 69/200, Train Loss: 0.1837, Test Loss: 0.3625\n",
      "Preds: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Labels: tensor([0., 0., 1., 0., 0., 0., 0., 1., 0., 0.])\n",
      "Epoch 70 Train Accuracy: 0.7343\n",
      "Epoch 70 Test Accuracy: 0.5294\n",
      "Epoch 70/200, Train Loss: 0.1868, Test Loss: 0.3035\n",
      "Preds: tensor([0., 1., 0., 1., 0., 1., 1., 0., 0., 1.])\n",
      "Labels: tensor([0., 1., 1., 1., 0., 1., 1., 0., 0., 1.])\n",
      "Epoch 71 Train Accuracy: 0.7421\n",
      "Epoch 71 Test Accuracy: 0.5275\n",
      "Epoch 71/200, Train Loss: 0.1905, Test Loss: 0.3595\n",
      "Preds: tensor([0., 0., 0., 0., 0., 1., 0., 0., 1., 1.])\n",
      "Labels: tensor([1., 0., 0., 0., 1., 1., 1., 0., 1., 1.])\n",
      "Epoch 72 Train Accuracy: 0.7465\n",
      "Epoch 72 Test Accuracy: 0.5372\n",
      "Epoch 72/200, Train Loss: 0.1777, Test Loss: 0.3818\n",
      "Preds: tensor([0., 1., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "Labels: tensor([0., 1., 0., 1., 1., 0., 1., 0., 0., 1.])\n",
      "Epoch 73 Train Accuracy: 0.7479\n",
      "Epoch 73 Test Accuracy: 0.5113\n",
      "Epoch 73/200, Train Loss: 0.1781, Test Loss: 0.3671\n",
      "Preds: tensor([1., 0., 0., 0., 1., 0., 0., 0., 1., 0.])\n",
      "Labels: tensor([1., 0., 0., 0., 1., 1., 0., 1., 1., 0.])\n",
      "Epoch 74 Train Accuracy: 0.7646\n",
      "Epoch 74 Test Accuracy: 0.5159\n",
      "Epoch 74/200, Train Loss: 0.1703, Test Loss: 0.4148\n",
      "Preds: tensor([0., 1., 1., 0., 1., 1., 0., 0., 1., 1.])\n",
      "Labels: tensor([0., 1., 1., 0., 1., 1., 1., 1., 1., 0.])\n",
      "Epoch 75 Train Accuracy: 0.7683\n",
      "Epoch 75 Test Accuracy: 0.5359\n",
      "Epoch 75/200, Train Loss: 0.1711, Test Loss: 0.3762\n",
      "Preds: tensor([1., 1., 0., 1., 1., 0., 1., 1., 0., 1.])\n",
      "Labels: tensor([0., 1., 0., 1., 0., 0., 1., 0., 0., 0.])\n",
      "Epoch 76 Train Accuracy: 0.7779\n",
      "Epoch 76 Test Accuracy: 0.5074\n",
      "Epoch 76/200, Train Loss: 0.1731, Test Loss: 0.3837\n",
      "Preds: tensor([1., 0., 1., 1., 0., 1., 0., 1., 1., 0.])\n",
      "Labels: tensor([1., 1., 1., 0., 0., 1., 0., 1., 1., 1.])\n",
      "Epoch 77 Train Accuracy: 0.7578\n",
      "Epoch 77 Test Accuracy: 0.5450\n",
      "Epoch 77/200, Train Loss: 0.1803, Test Loss: 0.3176\n",
      "Preds: tensor([1., 1., 0., 1., 1., 1., 0., 0., 1., 0.])\n",
      "Labels: tensor([0., 1., 1., 1., 1., 1., 0., 0., 1., 0.])\n",
      "Epoch 78 Train Accuracy: 0.7682\n",
      "Epoch 78 Test Accuracy: 0.5256\n",
      "Epoch 78/200, Train Loss: 0.1740, Test Loss: 0.3474\n",
      "Preds: tensor([1., 1., 0., 0., 1., 0., 0., 0., 1., 1.])\n",
      "Labels: tensor([0., 1., 1., 0., 1., 0., 0., 0., 1., 1.])\n",
      "Epoch 79 Train Accuracy: 0.7496\n",
      "Epoch 79 Test Accuracy: 0.4990\n",
      "Epoch 79/200, Train Loss: 0.1759, Test Loss: 0.3490\n",
      "Preds: tensor([0., 1., 0., 0., 1., 1., 1., 1., 0., 0.])\n",
      "Labels: tensor([0., 1., 0., 0., 1., 1., 1., 1., 0., 0.])\n",
      "Epoch 80 Train Accuracy: 0.7847\n",
      "Epoch 80 Test Accuracy: 0.5450\n",
      "Epoch 80/200, Train Loss: 0.1602, Test Loss: 0.4806\n",
      "Preds: tensor([0., 0., 1., 1., 0., 1., 1., 1., 0., 1.])\n",
      "Labels: tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])\n",
      "Epoch 81 Train Accuracy: 0.7776\n",
      "Epoch 81 Test Accuracy: 0.5023\n",
      "Epoch 81/200, Train Loss: 0.1633, Test Loss: 0.3702\n",
      "Preds: tensor([0., 1., 0., 0., 1., 0., 1., 0., 0., 0.])\n",
      "Labels: tensor([1., 1., 1., 0., 1., 0., 1., 1., 0., 0.])\n",
      "Epoch 82 Train Accuracy: 0.7928\n",
      "Epoch 82 Test Accuracy: 0.5146\n",
      "Epoch 82/200, Train Loss: 0.1566, Test Loss: 0.5525\n",
      "Preds: tensor([1., 0., 1., 0., 0., 1., 1., 1., 0., 1.])\n",
      "Labels: tensor([1., 0., 1., 0., 0., 0., 1., 1., 1., 1.])\n",
      "Epoch 83 Train Accuracy: 0.8025\n",
      "Epoch 83 Test Accuracy: 0.5437\n",
      "Epoch 83/200, Train Loss: 0.1559, Test Loss: 0.5836\n",
      "Preds: tensor([0., 0., 1., 0., 1., 0., 0., 1., 1., 1.])\n",
      "Labels: tensor([0., 0., 1., 1., 1., 0., 1., 1., 1., 1.])\n",
      "Epoch 84 Train Accuracy: 0.7751\n",
      "Epoch 84 Test Accuracy: 0.5476\n",
      "Epoch 84/200, Train Loss: 0.1759, Test Loss: 0.5808\n",
      "Preds: tensor([1., 1., 1., 0., 0., 1., 0., 1., 1., 0.])\n",
      "Labels: tensor([0., 0., 1., 0., 0., 1., 0., 0., 1., 0.])\n",
      "Epoch 85 Train Accuracy: 0.7763\n",
      "Epoch 85 Test Accuracy: 0.5417\n",
      "Epoch 85/200, Train Loss: 0.1627, Test Loss: 0.4954\n",
      "Preds: tensor([1., 0., 0., 1., 1., 0., 0., 1., 0., 1.])\n",
      "Labels: tensor([0., 0., 1., 1., 1., 0., 1., 1., 0., 1.])\n",
      "Epoch 86 Train Accuracy: 0.8072\n",
      "Epoch 86 Test Accuracy: 0.5359\n",
      "Epoch 86/200, Train Loss: 0.1534, Test Loss: 0.6113\n",
      "Preds: tensor([0., 0., 1., 0., 1., 1., 1., 0., 0., 1.])\n",
      "Labels: tensor([0., 0., 0., 0., 1., 1., 1., 1., 0., 1.])\n",
      "Epoch 87 Train Accuracy: 0.8080\n",
      "Epoch 87 Test Accuracy: 0.5029\n",
      "Epoch 87/200, Train Loss: 0.1541, Test Loss: 0.5325\n",
      "Preds: tensor([1., 1., 1., 1., 0., 1., 1., 0., 0., 1.])\n",
      "Labels: tensor([1., 1., 1., 1., 0., 0., 1., 1., 0., 1.])\n",
      "Epoch 88 Train Accuracy: 0.8216\n",
      "Epoch 88 Test Accuracy: 0.5172\n",
      "Epoch 88/200, Train Loss: 0.1412, Test Loss: 0.5714\n",
      "Preds: tensor([0., 0., 0., 1., 1., 0., 0., 0., 1., 0.])\n",
      "Labels: tensor([1., 0., 0., 1., 0., 0., 0., 0., 1., 0.])\n",
      "Epoch 89 Train Accuracy: 0.8232\n",
      "Epoch 89 Test Accuracy: 0.4971\n",
      "Epoch 89/200, Train Loss: 0.1406, Test Loss: 0.4841\n",
      "Preds: tensor([0., 0., 0., 1., 1., 0., 1., 1., 1., 1.])\n",
      "Labels: tensor([0., 1., 0., 1., 1., 0., 1., 0., 1., 0.])\n",
      "Epoch 90 Train Accuracy: 0.8078\n",
      "Epoch 90 Test Accuracy: 0.5133\n",
      "Epoch 90/200, Train Loss: 0.1496, Test Loss: 0.5998\n",
      "Preds: tensor([0., 0., 0., 1., 0., 0., 1., 0., 1., 1.])\n",
      "Labels: tensor([1., 1., 0., 1., 0., 0., 1., 0., 1., 1.])\n",
      "Epoch 91 Train Accuracy: 0.8078\n",
      "Epoch 91 Test Accuracy: 0.5282\n",
      "Epoch 91/200, Train Loss: 0.1514, Test Loss: 0.7881\n",
      "Preds: tensor([1., 0., 0., 1., 1., 0., 0., 0., 1., 1.])\n",
      "Labels: tensor([0., 0., 0., 1., 1., 1., 0., 0., 1., 1.])\n",
      "Epoch 92 Train Accuracy: 0.8020\n",
      "Epoch 92 Test Accuracy: 0.5223\n",
      "Epoch 92/200, Train Loss: 0.1510, Test Loss: 1.1273\n",
      "Preds: tensor([1., 0., 0., 1., 1., 0., 0., 1., 1., 0.])\n",
      "Labels: tensor([1., 0., 0., 1., 1., 0., 0., 1., 1., 0.])\n",
      "Epoch 93 Train Accuracy: 0.8036\n",
      "Epoch 93 Test Accuracy: 0.5243\n",
      "Epoch 93/200, Train Loss: 0.1492, Test Loss: 0.7080\n",
      "Preds: tensor([1., 0., 0., 0., 0., 0., 0., 1., 0., 1.])\n",
      "Labels: tensor([1., 0., 0., 0., 0., 0., 0., 1., 0., 1.])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[141]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Run\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     model = \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[140]\u001b[39m\u001b[32m, line 79\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     74\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     train_loss = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m     test_loss = evaluate(model, test_loader, criterion, device, writer, epoch)\n\u001b[32m     81\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Test Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[139]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, dataloader, optimizer, criterion, device, writer, epoch)\u001b[39m\n\u001b[32m      8\u001b[39m x1, x2, label = x1.to(device), x2.to(device), label.to(device)\n\u001b[32m      9\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m output = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m loss = criterion(output, label)\n\u001b[32m     12\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Python Projects\\NIPS2025\\venv311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Python Projects\\NIPS2025\\venv311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[138]\u001b[39m\u001b[32m, line 67\u001b[39m, in \u001b[36mSiameseNet.forward\u001b[39m\u001b[34m(self, x1, x2)\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x1, x2):\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     embed1 = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m     embed2 = \u001b[38;5;28mself\u001b[39m.encoder(x2)\n\u001b[32m     69\u001b[39m     distance = F.pairwise_distance(embed1, embed2)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Python Projects\\NIPS2025\\venv311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Python Projects\\NIPS2025\\venv311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[138]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mCNNEncoder.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.layers):\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m         x = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m     x = x.view(x.size(\u001b[32m0\u001b[39m), -\u001b[32m1\u001b[39m)\n\u001b[32m     39\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.fc(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Python Projects\\NIPS2025\\venv311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Python Projects\\NIPS2025\\venv311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Python Projects\\NIPS2025\\venv311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:375\u001b[39m, in \u001b[36mConv1d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    374\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Python Projects\\NIPS2025\\venv311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:370\u001b[39m, in \u001b[36mConv1d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    358\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    359\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv1d(\n\u001b[32m    360\u001b[39m         F.pad(\n\u001b[32m    361\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    368\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    369\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m370\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Run\n",
    "if __name__ == \"__main__\":\n",
    "    model = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50439854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24414"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebd612b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd8e416",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae17a80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfab46f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d560822",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a670a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
